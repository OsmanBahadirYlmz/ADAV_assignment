---
title: "Assignment - Part 1"
author: "Osman ..., Timo Scholts, Valerie Schilting"
date: "2024-05-14"
output:
  html_document:
    toc: true
    number_sections: true
    toc_float: true
    code_folding: hide
    theme: cosmo
---

# TO DO
(Here we can note what we have to do and who should do what.
This section should be deleted before handing it in, but it could be useful for tracking contributions to keep everything until the last moment.)

* Choose dataset and variables we want to use for the analysis
* **EDA:** Explore and learn about the structure of your data by constructing visualizations. Select a minimum of 2 and maximum of 3 graphs to illustrate your data in your final report.  
* **Regression analysis:** Based on the content of your data and the visualizations you constructed, formulate 1 research question that you will investigate using linear regression - data science style. That is, in the linear regression, make use of either best subset selection or shrinkage methods (Ridge regression or Lasso). Select the best linear model appropriately. To ensure reproducability of your findings, please make sure to use `set.seed()` in your R code. 


## Grading

    Quality and appropriateness of the data visualizations;
    Formulation of a fitting and clear research question;
    Quality of the performed regression analysis and analysis choices made;
    Quality of the interpretation of the model results and drawn conclusions;
    Quality of the R code;
    Overall quality of the report and presentation of the results.

## Research question
## hjhj

* y: base total stats (`base_total`)
* x possible: legendary status, exp growth, base happiness, base egg steps, capture rate


# Data setup (name should be changed later)

```{r, chunk options}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, library}
library(tidyverse)
```

```{r, data setup}
pokemon <- read_csv("data/pokemon_sort.csv")
data.frame(head(pokemon)) 
```


# Contributions per member
(Please state your contribution to the project here, make sure to also mention whether or not you used AI.
If you did use AI, mention which one and how you used it.
This section between parentheses should be deleted before handing in.)

## Osman

## Timo

```{r}
# the pokemon types have a certain color that has to be attached to it. this will be done by writing a feature that will attach the applicable hex color to the type name
colors <- c("normal" = "#A8A77A",
            "fire"= "#EE8130",
            "water"= "#6390F0",
            "electric"= "#F7D02C",
            "grass"= "#7AC74C",
            "ice"="#96D9D6",
            "fighting"= "#C22E28",
            "poison"= "#A33EA1",
            "ground"= "#E2BF65",
            "flying"= "#A98FF3",
            "psychic"= "#F95587",
            "bug"= "#A6B91A",
            "rock"= "#B6A136",
            "ghost"= "#735797",
            "dragon"= "#6F35FC",
            "dark"= "#705746",
            "steel"= "#B7B7CE",
            "fairy"= "#D685AD")
```
```{r}
#pokemon$type1 <- factor(
 #pokemon$type1,
  #levels = c("normal", "fire", "water", "electric", "grass", "ice", "fighting", "poison", "ground", "flying", "psychic", "bug", "rock", "ghost", "dragon", #"dark", "steel", "fairy")
#)
```

```{r}

library(dplyr)

# In order to compare the base total for each type it needs to be taken into account that pokemon can have two types. for this reason it was decided to duplicate the pokemon that have two types that it can be assigned to both types in the plot that will be made. This is done by filtering the pokemon that do not have NA. These filtered pokemon will then have type2 as the new type 1 and will then be binded with the original dataframe. 
pokemon_with_type2 <- pokemon %>%
  filter(!is.na(type2))

pokemon_with_duplicates_for_type <- pokemon_with_type2 %>%
  mutate(type1 = type2) %>%
  bind_rows(pokemon_with_type2)

pokemon_with_duplicates_for_type <- bind_rows(pokemon, duplicated_pokemon)


#this code is used to order the types according to the mean base total per type. This will order the types for the ggplot that will be made that will compare the base total per type. 
pokemon_with_duplicates_for_type$type1 <- factor(pokemon_with_duplicates_for_type$type1, levels = names(sort(tapply(pokemon_with_duplicates_for_type$base_total, pokemon_with_duplicates_for_type$type1, mean), decreasing = TRUE)))


# this code is used to make a violin plot. this will show the distribution per type of pokemon based on thier base total. 
violin_plot2 <- ggplot(pokemon_with_duplicates_for_type, aes(x = type1, y = base_total, fill = type1)) +
    geom_violin() +
  
    labs(
      title = "Violin plot of base total stats per type ordered by the mean base total of the pokemon for the type",
      x = "Types",  
      y = "Base Total",
      fill = colors,  
      caption = "Source:"
    ) +
    scale_fill_manual(
      values = colors) + theme_minimal()

violin_plot2


```
## OsmanBahadir Corelation matrix

```{r}
#delete numaric columns

pokemon <- read_csv("data/pokemon_sort.csv")
df<-pokemon
numeric_df <- df[sapply(df, is.numeric)]

# Calculate the correlation matrix
correlation_matrix <- cor(numeric_df)

# Print the correlation matrix
print(correlation_matrix)

view(correlation_matrix)

# Print the correlation matrix
print(correlation_matrix)

# Extract the 'base total' column
base_total_corr <- correlation_matrix[, "base_total"]

# Remove 'base total' self-correlation if present
base_total_corr <- base_total_corr[names(base_total_corr) != "base_total"]

# Sort the correlations
sorted_corr <- sort(base_total_corr, decreasing = TRUE)


# Load necessary library
library(ggplot2)

# Convert to data frame for ggplot
df <- data.frame(Parameter = names(sorted_corr), Correlation = sorted_corr)

# Create bar plot
ggplot(df, aes(x = reorder(Parameter, Correlation), y = Correlation)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Correlation with 'Base Total'", x = "Parameter", y = "Correlation") +
  theme_minimal()



# Add a column for color classification
df$Highlight <- "Normal"
df$Highlight[1:5] <- "Top 5"      # Top 5 correlations
df$Highlight[(nrow(df)-4):nrow(df)] <- "Bottom 5"  # Bottom 5 correlations

# Create a custom color palette
custom_colors <- c("Top 5" = "dodgerblue", "Bottom 5" = "firebrick", "Normal" = "grey70")

# Create bar plot
ggplot(df, aes(x = reorder(Parameter, Correlation), y = Correlation, fill = Highlight)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = custom_colors) +
  coord_flip() +
  labs(title = "Correlation with 'Base Total", x = "Parameter", y = "Correlation") +
  theme_minimal() +
  theme(legend.title = element_blank())



```
When we look at the graph, we can easily see that base total is highly correlated with sp_attack, attack, sp_defense, and HP. This is expected because the base total of a Pokémon is the sum of these properties. Therefore, we will delete these parameters from our analysis. Our goal is to use other features to calculate the base total of a Pokémon
```{r}

library(reshape2)

correlation_matrix <- cor(numeric_df)
correlation_df <- melt(correlation_matrix)
options(repr.plot.width = 20, repr.plot.height = 16)  # Set the size of the plot

# Plot the heatmap
ggplot(correlation_df, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name="Correlation") +
  theme_minimal() +
  labs(title = "Correlation HeatMap", x = "Var1", y = "Var2")+
  theme(axis.text.x = element_text(angle = 90, vjust = 1, 
                                   size = 6, hjust = 1),
        axis.text.y = element_text(size = 6)) +  # Adjust size of y-axis labels
  coord_fixed()


```


```{r}
#Data preProcessing

pokemon <- read_csv("data/pokemon_sort.csv")

# Handling the NaN values
nan_counts <- colSums(is.na(pokemon))
print(nan_counts)

library(dplyr)
# Function to fill NaN values with column mean
fill_na_with_mean <- function(df, column) {
  df %>% 
    mutate(!!sym(column) := ifelse(is.na(!!sym(column)), mean(!!sym(column), na.rm = TRUE), !!sym(column)))
}

# Columns to fill NaN values
columns_to_fill <- c("height_m", "weight_kg", "percentage_male")

# Fill NaN values for each specified column
for (col in columns_to_fill) {
  pokemon <- fill_na_with_mean(pokemon, col)
}

# Verify if NaN values are filled
sapply(pokemon[, columns_to_fill], function(x) sum(is.na(x)))

nan_counts <- colSums(is.na(pokemon))
print(nan_counts)




```
## data Preprocessing

```{r}
#Removing dummy columns
df<-(pokemon)

df <- select(df, -pokedex_number, -sp_attack, -sp_defense, -speed, -attack, -defense, -hp, -classfication, -japanese_name, -name, -abilities)


#Catagoric to numeric

#type1 and type2
# Fill NaN values in 'type2' column with 'only1type'
df$type2 <- ifelse(is.na(df$type2), 'only1type', df$type2)

# List all possible values
all_values <- c("poison", "flying", "dark", "electric", "ice", "ground", "fairy", 
                "grass", "fighting", "psychic", "steel", "fire", "rock", "water", 
                "dragon", "ghost", "bug", "normal", "only1type")

# Create new columns for each value and fill with 1 or 0 based on type1 and type2
for (val in all_values) {
  df <- df %>%
    mutate(!!val := as.numeric(type1 == val | type2 == val))
}


df <- df %>%
  select(-type1, -type2)

#converting chr column into numeric column
df$capture_rate <- ifelse(grepl("[^0-9.-]", df$capture_rate), NA, as.numeric(df$capture_rate))

view(df)




```


```{r}
#Test train split
library(caret)

set.seed(5462)

# define the training partition 
train_index <- createDataPartition(df$base_total, p = .5, 
                                  list = FALSE, 
                                  times = 1)

# split the data using the training partition to obtain training data
df_train <- df[train_index,]

# remainder of the split is the validation and test data (still) combined 
df_val_and_test <- df[-train_index,]

# split the remaining 50% of the data in a validation and test set
val_index <- createDataPartition(df_val_and_test$base_total, p = .6, 
                                  list = FALSE, 
                                  times = 1)

df_valid <- df_val_and_test[val_index,]
df_test  <- df_val_and_test[-val_index,]

view(df_train)
view(df_test)
view(df_valid)



```




```{r}
#Regression models for most correlated columns

lm_mse <- function(formula, train_data, valid_data) {
  y_name <- as.character(formula)[2]
  y_true <- valid_data[[y_name]]
  
  lm_fit <- lm(formula, train_data)
  y_pred <- predict(lm_fit, newdata = valid_data)
  
  mean((y_true - y_pred)^2)
}


lm_mse(base_total ~ height_m+base_egg_steps+is_legendary+weight_kg+experience_growth+dragon, df_train, df_valid)

```
```{r}

# Fit the model with all data
model <- lm(base_total ~ ., data = df_train)

summary(model)

```

```{r}

predictions_test <- predict(model, newdata = df_test)

predictions_valid <- predict(model, newdata = df_valid)

view(predictions_test)
view(predictions_valid)


# Function to calculate evaluation metrics
evaluate_model <- function(actual, predicted) {
  data.frame(
    MAE = mean(abs(actual - predicted)),
    MSE = mean((actual - predicted)^2),
    RMSE = sqrt(mean((actual - predicted)^2)),
    R2 = 1 - sum((actual - predicted)^2) / sum((actual - mean(actual))^2)
  )
}

# Evaluate on test set
test_evaluation <- evaluate_model(df_test$base_total, predictions_test)
print(test_evaluation)

# Evaluate on validation set
valid_evaluation <- evaluate_model(df_valid$base_total, predictions_valid)
print(valid_evaluation)


```



```{r}


```


















































